{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "QuantifyAL_BioBERT.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Binary Classification using BIO BERT\n",
        "This code is modified and adapted based on the repository\n",
        "https://github.com/perkdrew/advanced-nlp/blob/master/BioBERT/classification/biobert_classification.ipynb"
      ],
      "metadata": {
        "id": "6RvtEFatBQNF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Get the BioBERT Weights"
      ],
      "metadata": {
        "id": "D2zDd8_yCRiJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-mfyjIdKBPch",
        "outputId": "eb6aa35f-5c0f-4c8d-e4d1-1a1abf79debb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-05-30 22:31:22--  https://docs.google.com/uc?export=download&confirm=t&id=1R84voFKHfWV9xjzeLzWBbmY1uOMYpnyD\n",
            "Resolving docs.google.com (docs.google.com)... 142.251.8.101, 142.251.8.102, 142.251.8.113, ...\n",
            "Connecting to docs.google.com (docs.google.com)|142.251.8.101|:443... connected.\n",
            "HTTP request sent, awaiting response... 303 See Other\n",
            "Location: https://doc-10-20-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/hqdoq12gcdtcs5adji2qe67l15751q5v/1653949875000/13799006341648886493/*/1R84voFKHfWV9xjzeLzWBbmY1uOMYpnyD?e=download [following]\n",
            "Warning: wildcards not supported in HTTP.\n",
            "--2022-05-30 22:31:22--  https://doc-10-20-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/hqdoq12gcdtcs5adji2qe67l15751q5v/1653949875000/13799006341648886493/*/1R84voFKHfWV9xjzeLzWBbmY1uOMYpnyD?e=download\n",
            "Resolving doc-10-20-docs.googleusercontent.com (doc-10-20-docs.googleusercontent.com)... 108.177.125.132, 2404:6800:4008:c01::84\n",
            "Connecting to doc-10-20-docs.googleusercontent.com (doc-10-20-docs.googleusercontent.com)|108.177.125.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 401403346 (383M) [application/x-gzip]\n",
            "Saving to: ‘biobert_weights’\n",
            "\n",
            "biobert_weights     100%[===================>] 382.81M  71.0MB/s    in 5.6s    \n",
            "\n",
            "2022-05-30 22:31:29 (68.2 MB/s) - ‘biobert_weights’ saved [401403346/401403346]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget --load-cookies /tmp/cookies.txt \"https://docs.google.com/uc?export=download&confirm=$(wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate 'https://docs.google.com/uc?export=download&id=1R84voFKHfWV9xjzeLzWBbmY1uOMYpnyD' -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\\1\\n/p')&id=1R84voFKHfWV9xjzeLzWBbmY1uOMYpnyD\" -O biobert_weights && rm -rf /tmp/cookies.txt\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!tar -xzf biobert_weights\n",
        "!ls biobert_v1.1_pubmed/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6aYMVGZrCiZP",
        "outputId": "fe5445f3-064d-4437-a756-636c1d806752"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "bert_config.json\t\t\tmodel.ckpt-1000000.index  vocab.txt\n",
            "model.ckpt-1000000.data-00000-of-00001\tmodel.ckpt-1000000.meta\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Install required libraries"
      ],
      "metadata": {
        "id": "8awhGOY2CWlG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pytorch_transformers\n",
        "!pip install transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rbIcnPgTCV6L",
        "outputId": "9b4d2b8b-05fa-45c3-c913-809bf0d1fd04"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pytorch_transformers\n",
            "  Downloading pytorch_transformers-1.2.0-py3-none-any.whl (176 kB)\n",
            "\u001b[K     |████████████████████████████████| 176 kB 4.2 MB/s \n",
            "\u001b[?25hCollecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.96-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2 MB 53.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from pytorch_transformers) (2019.12.20)\n",
            "Collecting sacremoses\n",
            "  Downloading sacremoses-0.0.53.tar.gz (880 kB)\n",
            "\u001b[K     |████████████████████████████████| 880 kB 50.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from pytorch_transformers) (4.64.0)\n",
            "Collecting boto3\n",
            "  Downloading boto3-1.23.10-py3-none-any.whl (132 kB)\n",
            "\u001b[K     |████████████████████████████████| 132 kB 71.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from pytorch_transformers) (1.11.0+cu113)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from pytorch_transformers) (1.21.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from pytorch_transformers) (2.23.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.0.0->pytorch_transformers) (4.2.0)\n",
            "Collecting jmespath<2.0.0,>=0.7.1\n",
            "  Downloading jmespath-1.0.0-py3-none-any.whl (23 kB)\n",
            "Collecting s3transfer<0.6.0,>=0.5.0\n",
            "  Downloading s3transfer-0.5.2-py3-none-any.whl (79 kB)\n",
            "\u001b[K     |████████████████████████████████| 79 kB 9.6 MB/s \n",
            "\u001b[?25hCollecting botocore<1.27.0,>=1.26.10\n",
            "  Downloading botocore-1.26.10-py3-none-any.whl (8.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 8.8 MB 61.8 MB/s \n",
            "\u001b[?25hCollecting urllib3<1.27,>=1.25.4\n",
            "  Downloading urllib3-1.26.9-py2.py3-none-any.whl (138 kB)\n",
            "\u001b[K     |████████████████████████████████| 138 kB 78.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.7/dist-packages (from botocore<1.27.0,>=1.26.10->boto3->pytorch_transformers) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.27.0,>=1.26.10->boto3->pytorch_transformers) (1.15.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->pytorch_transformers) (2022.5.18.1)\n",
            "  Downloading urllib3-1.25.11-py2.py3-none-any.whl (127 kB)\n",
            "\u001b[K     |████████████████████████████████| 127 kB 75.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->pytorch_transformers) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->pytorch_transformers) (2.10)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->pytorch_transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->pytorch_transformers) (1.1.0)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.53-py3-none-any.whl size=895260 sha256=1ce244a335255c6da3747189b2b21e99178c2c4a537d8297b62eb545c07edd14\n",
            "  Stored in directory: /root/.cache/pip/wheels/87/39/dd/a83eeef36d0bf98e7a4d1933a4ad2d660295a40613079bafc9\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: urllib3, jmespath, botocore, s3transfer, sentencepiece, sacremoses, boto3, pytorch-transformers\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 1.24.3\n",
            "    Uninstalling urllib3-1.24.3:\n",
            "      Successfully uninstalled urllib3-1.24.3\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "Successfully installed boto3-1.23.10 botocore-1.26.10 jmespath-1.0.0 pytorch-transformers-1.2.0 s3transfer-0.5.2 sacremoses-0.0.53 sentencepiece-0.1.96 urllib3-1.25.11\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.19.2-py3-none-any.whl (4.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.2 MB 4.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Collecting tokenizers!=0.11.3,<0.13,>=0.11.1\n",
            "  Downloading tokenizers-0.12.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.6 MB 64.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.7.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.11.3)\n",
            "Collecting pyyaml>=5.1\n",
            "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
            "\u001b[K     |████████████████████████████████| 596 kB 72.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Collecting huggingface-hub<1.0,>=0.1.0\n",
            "  Downloading huggingface_hub-0.7.0-py3-none-any.whl (86 kB)\n",
            "\u001b[K     |████████████████████████████████| 86 kB 5.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.2.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.8.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2022.5.18.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.25.11)\n",
            "Installing collected packages: pyyaml, tokenizers, huggingface-hub, transformers\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed huggingface-hub-0.7.0 pyyaml-6.0 tokenizers-0.12.1 transformers-4.19.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# install BERT\n",
        "!pip install pytorch_pretrained_bert pytorch-nlp"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DA42JH4FFsHu",
        "outputId": "5b12b115-b8eb-47d5-b105-eade31986f82"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pytorch_pretrained_bert\n",
            "  Downloading pytorch_pretrained_bert-0.6.2-py3-none-any.whl (123 kB)\n",
            "\u001b[K     |████████████████████████████████| 123 kB 4.3 MB/s \n",
            "\u001b[?25hCollecting pytorch-nlp\n",
            "  Downloading pytorch_nlp-0.5.0-py3-none-any.whl (90 kB)\n",
            "\u001b[K     |████████████████████████████████| 90 kB 8.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from pytorch_pretrained_bert) (2019.12.20)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from pytorch_pretrained_bert) (4.64.0)\n",
            "Requirement already satisfied: torch>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from pytorch_pretrained_bert) (1.11.0+cu113)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from pytorch_pretrained_bert) (1.21.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from pytorch_pretrained_bert) (2.23.0)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.7/dist-packages (from pytorch_pretrained_bert) (1.23.10)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=0.4.1->pytorch_pretrained_bert) (4.2.0)\n",
            "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /usr/local/lib/python3.7/dist-packages (from boto3->pytorch_pretrained_bert) (1.0.0)\n",
            "Requirement already satisfied: botocore<1.27.0,>=1.26.10 in /usr/local/lib/python3.7/dist-packages (from boto3->pytorch_pretrained_bert) (1.26.10)\n",
            "Requirement already satisfied: s3transfer<0.6.0,>=0.5.0 in /usr/local/lib/python3.7/dist-packages (from boto3->pytorch_pretrained_bert) (0.5.2)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.7/dist-packages (from botocore<1.27.0,>=1.26.10->boto3->pytorch_pretrained_bert) (2.8.2)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.25.4 in /usr/local/lib/python3.7/dist-packages (from botocore<1.27.0,>=1.26.10->boto3->pytorch_pretrained_bert) (1.25.11)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.27.0,>=1.26.10->boto3->pytorch_pretrained_bert) (1.15.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->pytorch_pretrained_bert) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->pytorch_pretrained_bert) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->pytorch_pretrained_bert) (2022.5.18.1)\n",
            "Installing collected packages: pytorch-pretrained-bert, pytorch-nlp\n",
            "Successfully installed pytorch-nlp-0.5.0 pytorch-pretrained-bert-0.6.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "from cf_matrix import make_confusion_matrix \n",
        "import dfutilities\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm, trange\n",
        "\n",
        "# BERT imports\n",
        "import torch\n",
        "from pytorch_transformers import BertModel\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from pytorch_pretrained_bert import BertTokenizer, BertConfig\n",
        "from pytorch_pretrained_bert import BertAdam, BertForSequenceClassification"
      ],
      "metadata": {
        "id": "mVpJiwbUEMeh"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# specify GPU device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "n_gpu = torch.cuda.device_count()\n",
        "torch.cuda.get_device_name(0)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "30w6hFfmF33P",
        "outputId": "31d217f9-9e98-4552-a49c-f6b483273efa"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Tesla P100-PCIE-16GB'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Create the model using BioBERT Pretrained Weights"
      ],
      "metadata": {
        "id": "zS3WdMC5Cz1h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!transformers-cli convert --model_type bert --tf_checkpoint biobert_v1.1_pubmed/model.ckpt-1000000 --config biobert_v1.1_pubmed/bert_config.json --pytorch_dump_output biobert_v1.1_pubmed/pytorch_model.bin\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DMOeq4FrDD9c",
        "outputId": "676801ce-41e6-45e8-c939-b17154f86d76"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Building PyTorch model from configuration: BertConfig {\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.19.2\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n",
            "Converting TensorFlow checkpoint from /content/biobert_v1.1_pubmed/model.ckpt-1000000\n",
            "Loading TF weight bert/embeddings/LayerNorm/beta with shape [768]\n",
            "Loading TF weight bert/embeddings/LayerNorm/gamma with shape [768]\n",
            "Loading TF weight bert/embeddings/position_embeddings with shape [512, 768]\n",
            "Loading TF weight bert/embeddings/token_type_embeddings with shape [2, 768]\n",
            "Loading TF weight bert/embeddings/word_embeddings with shape [28996, 768]\n",
            "Loading TF weight bert/encoder/layer_0/attention/output/LayerNorm/beta with shape [768]\n",
            "Loading TF weight bert/encoder/layer_0/attention/output/LayerNorm/gamma with shape [768]\n",
            "Loading TF weight bert/encoder/layer_0/attention/output/dense/bias with shape [768]\n",
            "Loading TF weight bert/encoder/layer_0/attention/output/dense/kernel with shape [768, 768]\n",
            "Loading TF weight bert/encoder/layer_0/attention/self/key/bias with shape [768]\n",
            "Loading TF weight bert/encoder/layer_0/attention/self/key/kernel with shape [768, 768]\n",
            "Loading TF weight bert/encoder/layer_0/attention/self/query/bias with shape [768]\n",
            "Loading TF weight bert/encoder/layer_0/attention/self/query/kernel with shape [768, 768]\n",
            "Loading TF weight bert/encoder/layer_0/attention/self/value/bias with shape [768]\n",
            "Loading TF weight bert/encoder/layer_0/attention/self/value/kernel with shape [768, 768]\n",
            "Loading TF weight bert/encoder/layer_0/intermediate/dense/bias with shape [3072]\n",
            "Loading TF weight bert/encoder/layer_0/intermediate/dense/kernel with shape [768, 3072]\n",
            "Loading TF weight bert/encoder/layer_0/output/LayerNorm/beta with shape [768]\n",
            "Loading TF weight bert/encoder/layer_0/output/LayerNorm/gamma with shape [768]\n",
            "Loading TF weight bert/encoder/layer_0/output/dense/bias with shape [768]\n",
            "Loading TF weight bert/encoder/layer_0/output/dense/kernel with shape [3072, 768]\n",
            "Loading TF weight bert/encoder/layer_1/attention/output/LayerNorm/beta with shape [768]\n",
            "Loading TF weight bert/encoder/layer_1/attention/output/LayerNorm/gamma with shape [768]\n",
            "Loading TF weight bert/encoder/layer_1/attention/output/dense/bias with shape [768]\n",
            "Loading TF weight bert/encoder/layer_1/attention/output/dense/kernel with shape [768, 768]\n",
            "Loading TF weight bert/encoder/layer_1/attention/self/key/bias with shape [768]\n",
            "Loading TF weight bert/encoder/layer_1/attention/self/key/kernel with shape [768, 768]\n",
            "Loading TF weight bert/encoder/layer_1/attention/self/query/bias with shape [768]\n",
            "Loading TF weight bert/encoder/layer_1/attention/self/query/kernel with shape [768, 768]\n",
            "Loading TF weight bert/encoder/layer_1/attention/self/value/bias with shape [768]\n",
            "Loading TF weight bert/encoder/layer_1/attention/self/value/kernel with shape [768, 768]\n",
            "Loading TF weight bert/encoder/layer_1/intermediate/dense/bias with shape [3072]\n",
            "Loading TF weight bert/encoder/layer_1/intermediate/dense/kernel with shape [768, 3072]\n",
            "Loading TF weight bert/encoder/layer_1/output/LayerNorm/beta with shape [768]\n",
            "Loading TF weight bert/encoder/layer_1/output/LayerNorm/gamma with shape [768]\n",
            "Loading TF weight bert/encoder/layer_1/output/dense/bias with shape [768]\n",
            "Loading TF weight bert/encoder/layer_1/output/dense/kernel with shape [3072, 768]\n",
            "Loading TF weight bert/encoder/layer_10/attention/output/LayerNorm/beta with shape [768]\n",
            "Loading TF weight bert/encoder/layer_10/attention/output/LayerNorm/gamma with shape [768]\n",
            "Loading TF weight bert/encoder/layer_10/attention/output/dense/bias with shape [768]\n",
            "Loading TF weight bert/encoder/layer_10/attention/output/dense/kernel with shape [768, 768]\n",
            "Loading TF weight bert/encoder/layer_10/attention/self/key/bias with shape [768]\n",
            "Loading TF weight bert/encoder/layer_10/attention/self/key/kernel with shape [768, 768]\n",
            "Loading TF weight bert/encoder/layer_10/attention/self/query/bias with shape [768]\n",
            "Loading TF weight bert/encoder/layer_10/attention/self/query/kernel with shape [768, 768]\n",
            "Loading TF weight bert/encoder/layer_10/attention/self/value/bias with shape [768]\n",
            "Loading TF weight bert/encoder/layer_10/attention/self/value/kernel with shape [768, 768]\n",
            "Loading TF weight bert/encoder/layer_10/intermediate/dense/bias with shape [3072]\n",
            "Loading TF weight bert/encoder/layer_10/intermediate/dense/kernel with shape [768, 3072]\n",
            "Loading TF weight bert/encoder/layer_10/output/LayerNorm/beta with shape [768]\n",
            "Loading TF weight bert/encoder/layer_10/output/LayerNorm/gamma with shape [768]\n",
            "Loading TF weight bert/encoder/layer_10/output/dense/bias with shape [768]\n",
            "Loading TF weight bert/encoder/layer_10/output/dense/kernel with shape [3072, 768]\n",
            "Loading TF weight bert/encoder/layer_11/attention/output/LayerNorm/beta with shape [768]\n",
            "Loading TF weight bert/encoder/layer_11/attention/output/LayerNorm/gamma with shape [768]\n",
            "Loading TF weight bert/encoder/layer_11/attention/output/dense/bias with shape [768]\n",
            "Loading TF weight bert/encoder/layer_11/attention/output/dense/kernel with shape [768, 768]\n",
            "Loading TF weight bert/encoder/layer_11/attention/self/key/bias with shape [768]\n",
            "Loading TF weight bert/encoder/layer_11/attention/self/key/kernel with shape [768, 768]\n",
            "Loading TF weight bert/encoder/layer_11/attention/self/query/bias with shape [768]\n",
            "Loading TF weight bert/encoder/layer_11/attention/self/query/kernel with shape [768, 768]\n",
            "Loading TF weight bert/encoder/layer_11/attention/self/value/bias with shape [768]\n",
            "Loading TF weight bert/encoder/layer_11/attention/self/value/kernel with shape [768, 768]\n",
            "Loading TF weight bert/encoder/layer_11/intermediate/dense/bias with shape [3072]\n",
            "Loading TF weight bert/encoder/layer_11/intermediate/dense/kernel with shape [768, 3072]\n",
            "Loading TF weight bert/encoder/layer_11/output/LayerNorm/beta with shape [768]\n",
            "Loading TF weight bert/encoder/layer_11/output/LayerNorm/gamma with shape [768]\n",
            "Loading TF weight bert/encoder/layer_11/output/dense/bias with shape [768]\n",
            "Loading TF weight bert/encoder/layer_11/output/dense/kernel with shape [3072, 768]\n",
            "Loading TF weight bert/encoder/layer_2/attention/output/LayerNorm/beta with shape [768]\n",
            "Loading TF weight bert/encoder/layer_2/attention/output/LayerNorm/gamma with shape [768]\n",
            "Loading TF weight bert/encoder/layer_2/attention/output/dense/bias with shape [768]\n",
            "Loading TF weight bert/encoder/layer_2/attention/output/dense/kernel with shape [768, 768]\n",
            "Loading TF weight bert/encoder/layer_2/attention/self/key/bias with shape [768]\n",
            "Loading TF weight bert/encoder/layer_2/attention/self/key/kernel with shape [768, 768]\n",
            "Loading TF weight bert/encoder/layer_2/attention/self/query/bias with shape [768]\n",
            "Loading TF weight bert/encoder/layer_2/attention/self/query/kernel with shape [768, 768]\n",
            "Loading TF weight bert/encoder/layer_2/attention/self/value/bias with shape [768]\n",
            "Loading TF weight bert/encoder/layer_2/attention/self/value/kernel with shape [768, 768]\n",
            "Loading TF weight bert/encoder/layer_2/intermediate/dense/bias with shape [3072]\n",
            "Loading TF weight bert/encoder/layer_2/intermediate/dense/kernel with shape [768, 3072]\n",
            "Loading TF weight bert/encoder/layer_2/output/LayerNorm/beta with shape [768]\n",
            "Loading TF weight bert/encoder/layer_2/output/LayerNorm/gamma with shape [768]\n",
            "Loading TF weight bert/encoder/layer_2/output/dense/bias with shape [768]\n",
            "Loading TF weight bert/encoder/layer_2/output/dense/kernel with shape [3072, 768]\n",
            "Loading TF weight bert/encoder/layer_3/attention/output/LayerNorm/beta with shape [768]\n",
            "Loading TF weight bert/encoder/layer_3/attention/output/LayerNorm/gamma with shape [768]\n",
            "Loading TF weight bert/encoder/layer_3/attention/output/dense/bias with shape [768]\n",
            "Loading TF weight bert/encoder/layer_3/attention/output/dense/kernel with shape [768, 768]\n",
            "Loading TF weight bert/encoder/layer_3/attention/self/key/bias with shape [768]\n",
            "Loading TF weight bert/encoder/layer_3/attention/self/key/kernel with shape [768, 768]\n",
            "Loading TF weight bert/encoder/layer_3/attention/self/query/bias with shape [768]\n",
            "Loading TF weight bert/encoder/layer_3/attention/self/query/kernel with shape [768, 768]\n",
            "Loading TF weight bert/encoder/layer_3/attention/self/value/bias with shape [768]\n",
            "Loading TF weight bert/encoder/layer_3/attention/self/value/kernel with shape [768, 768]\n",
            "Loading TF weight bert/encoder/layer_3/intermediate/dense/bias with shape [3072]\n",
            "Loading TF weight bert/encoder/layer_3/intermediate/dense/kernel with shape [768, 3072]\n",
            "Loading TF weight bert/encoder/layer_3/output/LayerNorm/beta with shape [768]\n",
            "Loading TF weight bert/encoder/layer_3/output/LayerNorm/gamma with shape [768]\n",
            "Loading TF weight bert/encoder/layer_3/output/dense/bias with shape [768]\n",
            "Loading TF weight bert/encoder/layer_3/output/dense/kernel with shape [3072, 768]\n",
            "Loading TF weight bert/encoder/layer_4/attention/output/LayerNorm/beta with shape [768]\n",
            "Loading TF weight bert/encoder/layer_4/attention/output/LayerNorm/gamma with shape [768]\n",
            "Loading TF weight bert/encoder/layer_4/attention/output/dense/bias with shape [768]\n",
            "Loading TF weight bert/encoder/layer_4/attention/output/dense/kernel with shape [768, 768]\n",
            "Loading TF weight bert/encoder/layer_4/attention/self/key/bias with shape [768]\n",
            "Loading TF weight bert/encoder/layer_4/attention/self/key/kernel with shape [768, 768]\n",
            "Loading TF weight bert/encoder/layer_4/attention/self/query/bias with shape [768]\n",
            "Loading TF weight bert/encoder/layer_4/attention/self/query/kernel with shape [768, 768]\n",
            "Loading TF weight bert/encoder/layer_4/attention/self/value/bias with shape [768]\n",
            "Loading TF weight bert/encoder/layer_4/attention/self/value/kernel with shape [768, 768]\n",
            "Loading TF weight bert/encoder/layer_4/intermediate/dense/bias with shape [3072]\n",
            "Loading TF weight bert/encoder/layer_4/intermediate/dense/kernel with shape [768, 3072]\n",
            "Loading TF weight bert/encoder/layer_4/output/LayerNorm/beta with shape [768]\n",
            "Loading TF weight bert/encoder/layer_4/output/LayerNorm/gamma with shape [768]\n",
            "Loading TF weight bert/encoder/layer_4/output/dense/bias with shape [768]\n",
            "Loading TF weight bert/encoder/layer_4/output/dense/kernel with shape [3072, 768]\n",
            "Loading TF weight bert/encoder/layer_5/attention/output/LayerNorm/beta with shape [768]\n",
            "Loading TF weight bert/encoder/layer_5/attention/output/LayerNorm/gamma with shape [768]\n",
            "Loading TF weight bert/encoder/layer_5/attention/output/dense/bias with shape [768]\n",
            "Loading TF weight bert/encoder/layer_5/attention/output/dense/kernel with shape [768, 768]\n",
            "Loading TF weight bert/encoder/layer_5/attention/self/key/bias with shape [768]\n",
            "Loading TF weight bert/encoder/layer_5/attention/self/key/kernel with shape [768, 768]\n",
            "Loading TF weight bert/encoder/layer_5/attention/self/query/bias with shape [768]\n",
            "Loading TF weight bert/encoder/layer_5/attention/self/query/kernel with shape [768, 768]\n",
            "Loading TF weight bert/encoder/layer_5/attention/self/value/bias with shape [768]\n",
            "Loading TF weight bert/encoder/layer_5/attention/self/value/kernel with shape [768, 768]\n",
            "Loading TF weight bert/encoder/layer_5/intermediate/dense/bias with shape [3072]\n",
            "Loading TF weight bert/encoder/layer_5/intermediate/dense/kernel with shape [768, 3072]\n",
            "Loading TF weight bert/encoder/layer_5/output/LayerNorm/beta with shape [768]\n",
            "Loading TF weight bert/encoder/layer_5/output/LayerNorm/gamma with shape [768]\n",
            "Loading TF weight bert/encoder/layer_5/output/dense/bias with shape [768]\n",
            "Loading TF weight bert/encoder/layer_5/output/dense/kernel with shape [3072, 768]\n",
            "Loading TF weight bert/encoder/layer_6/attention/output/LayerNorm/beta with shape [768]\n",
            "Loading TF weight bert/encoder/layer_6/attention/output/LayerNorm/gamma with shape [768]\n",
            "Loading TF weight bert/encoder/layer_6/attention/output/dense/bias with shape [768]\n",
            "Loading TF weight bert/encoder/layer_6/attention/output/dense/kernel with shape [768, 768]\n",
            "Loading TF weight bert/encoder/layer_6/attention/self/key/bias with shape [768]\n",
            "Loading TF weight bert/encoder/layer_6/attention/self/key/kernel with shape [768, 768]\n",
            "Loading TF weight bert/encoder/layer_6/attention/self/query/bias with shape [768]\n",
            "Loading TF weight bert/encoder/layer_6/attention/self/query/kernel with shape [768, 768]\n",
            "Loading TF weight bert/encoder/layer_6/attention/self/value/bias with shape [768]\n",
            "Loading TF weight bert/encoder/layer_6/attention/self/value/kernel with shape [768, 768]\n",
            "Loading TF weight bert/encoder/layer_6/intermediate/dense/bias with shape [3072]\n",
            "Loading TF weight bert/encoder/layer_6/intermediate/dense/kernel with shape [768, 3072]\n",
            "Loading TF weight bert/encoder/layer_6/output/LayerNorm/beta with shape [768]\n",
            "Loading TF weight bert/encoder/layer_6/output/LayerNorm/gamma with shape [768]\n",
            "Loading TF weight bert/encoder/layer_6/output/dense/bias with shape [768]\n",
            "Loading TF weight bert/encoder/layer_6/output/dense/kernel with shape [3072, 768]\n",
            "Loading TF weight bert/encoder/layer_7/attention/output/LayerNorm/beta with shape [768]\n",
            "Loading TF weight bert/encoder/layer_7/attention/output/LayerNorm/gamma with shape [768]\n",
            "Loading TF weight bert/encoder/layer_7/attention/output/dense/bias with shape [768]\n",
            "Loading TF weight bert/encoder/layer_7/attention/output/dense/kernel with shape [768, 768]\n",
            "Loading TF weight bert/encoder/layer_7/attention/self/key/bias with shape [768]\n",
            "Loading TF weight bert/encoder/layer_7/attention/self/key/kernel with shape [768, 768]\n",
            "Loading TF weight bert/encoder/layer_7/attention/self/query/bias with shape [768]\n",
            "Loading TF weight bert/encoder/layer_7/attention/self/query/kernel with shape [768, 768]\n",
            "Loading TF weight bert/encoder/layer_7/attention/self/value/bias with shape [768]\n",
            "Loading TF weight bert/encoder/layer_7/attention/self/value/kernel with shape [768, 768]\n",
            "Loading TF weight bert/encoder/layer_7/intermediate/dense/bias with shape [3072]\n",
            "Loading TF weight bert/encoder/layer_7/intermediate/dense/kernel with shape [768, 3072]\n",
            "Loading TF weight bert/encoder/layer_7/output/LayerNorm/beta with shape [768]\n",
            "Loading TF weight bert/encoder/layer_7/output/LayerNorm/gamma with shape [768]\n",
            "Loading TF weight bert/encoder/layer_7/output/dense/bias with shape [768]\n",
            "Loading TF weight bert/encoder/layer_7/output/dense/kernel with shape [3072, 768]\n",
            "Loading TF weight bert/encoder/layer_8/attention/output/LayerNorm/beta with shape [768]\n",
            "Loading TF weight bert/encoder/layer_8/attention/output/LayerNorm/gamma with shape [768]\n",
            "Loading TF weight bert/encoder/layer_8/attention/output/dense/bias with shape [768]\n",
            "Loading TF weight bert/encoder/layer_8/attention/output/dense/kernel with shape [768, 768]\n",
            "Loading TF weight bert/encoder/layer_8/attention/self/key/bias with shape [768]\n",
            "Loading TF weight bert/encoder/layer_8/attention/self/key/kernel with shape [768, 768]\n",
            "Loading TF weight bert/encoder/layer_8/attention/self/query/bias with shape [768]\n",
            "Loading TF weight bert/encoder/layer_8/attention/self/query/kernel with shape [768, 768]\n",
            "Loading TF weight bert/encoder/layer_8/attention/self/value/bias with shape [768]\n",
            "Loading TF weight bert/encoder/layer_8/attention/self/value/kernel with shape [768, 768]\n",
            "Loading TF weight bert/encoder/layer_8/intermediate/dense/bias with shape [3072]\n",
            "Loading TF weight bert/encoder/layer_8/intermediate/dense/kernel with shape [768, 3072]\n",
            "Loading TF weight bert/encoder/layer_8/output/LayerNorm/beta with shape [768]\n",
            "Loading TF weight bert/encoder/layer_8/output/LayerNorm/gamma with shape [768]\n",
            "Loading TF weight bert/encoder/layer_8/output/dense/bias with shape [768]\n",
            "Loading TF weight bert/encoder/layer_8/output/dense/kernel with shape [3072, 768]\n",
            "Loading TF weight bert/encoder/layer_9/attention/output/LayerNorm/beta with shape [768]\n",
            "Loading TF weight bert/encoder/layer_9/attention/output/LayerNorm/gamma with shape [768]\n",
            "Loading TF weight bert/encoder/layer_9/attention/output/dense/bias with shape [768]\n",
            "Loading TF weight bert/encoder/layer_9/attention/output/dense/kernel with shape [768, 768]\n",
            "Loading TF weight bert/encoder/layer_9/attention/self/key/bias with shape [768]\n",
            "Loading TF weight bert/encoder/layer_9/attention/self/key/kernel with shape [768, 768]\n",
            "Loading TF weight bert/encoder/layer_9/attention/self/query/bias with shape [768]\n",
            "Loading TF weight bert/encoder/layer_9/attention/self/query/kernel with shape [768, 768]\n",
            "Loading TF weight bert/encoder/layer_9/attention/self/value/bias with shape [768]\n",
            "Loading TF weight bert/encoder/layer_9/attention/self/value/kernel with shape [768, 768]\n",
            "Loading TF weight bert/encoder/layer_9/intermediate/dense/bias with shape [3072]\n",
            "Loading TF weight bert/encoder/layer_9/intermediate/dense/kernel with shape [768, 3072]\n",
            "Loading TF weight bert/encoder/layer_9/output/LayerNorm/beta with shape [768]\n",
            "Loading TF weight bert/encoder/layer_9/output/LayerNorm/gamma with shape [768]\n",
            "Loading TF weight bert/encoder/layer_9/output/dense/bias with shape [768]\n",
            "Loading TF weight bert/encoder/layer_9/output/dense/kernel with shape [3072, 768]\n",
            "Loading TF weight bert/pooler/dense/bias with shape [768]\n",
            "Loading TF weight bert/pooler/dense/kernel with shape [768, 768]\n",
            "Initialize PyTorch weight ['bert', 'embeddings', 'LayerNorm', 'beta']\n",
            "Initialize PyTorch weight ['bert', 'embeddings', 'LayerNorm', 'gamma']\n",
            "Initialize PyTorch weight ['bert', 'embeddings', 'position_embeddings']\n",
            "Initialize PyTorch weight ['bert', 'embeddings', 'token_type_embeddings']\n",
            "Initialize PyTorch weight ['bert', 'embeddings', 'word_embeddings']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'attention', 'output', 'LayerNorm', 'beta']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'attention', 'output', 'LayerNorm', 'gamma']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'attention', 'output', 'dense', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'attention', 'output', 'dense', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'attention', 'self', 'key', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'attention', 'self', 'key', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'attention', 'self', 'query', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'attention', 'self', 'query', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'attention', 'self', 'value', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'attention', 'self', 'value', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'intermediate', 'dense', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'intermediate', 'dense', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'output', 'LayerNorm', 'beta']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'output', 'LayerNorm', 'gamma']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'output', 'dense', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'output', 'dense', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'attention', 'output', 'LayerNorm', 'beta']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'attention', 'output', 'LayerNorm', 'gamma']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'attention', 'output', 'dense', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'attention', 'output', 'dense', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'attention', 'self', 'key', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'attention', 'self', 'key', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'attention', 'self', 'query', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'attention', 'self', 'query', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'attention', 'self', 'value', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'attention', 'self', 'value', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'intermediate', 'dense', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'intermediate', 'dense', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'output', 'LayerNorm', 'beta']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'output', 'LayerNorm', 'gamma']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'output', 'dense', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'output', 'dense', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_10', 'attention', 'output', 'LayerNorm', 'beta']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_10', 'attention', 'output', 'LayerNorm', 'gamma']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_10', 'attention', 'output', 'dense', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_10', 'attention', 'output', 'dense', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_10', 'attention', 'self', 'key', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_10', 'attention', 'self', 'key', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_10', 'attention', 'self', 'query', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_10', 'attention', 'self', 'query', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_10', 'attention', 'self', 'value', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_10', 'attention', 'self', 'value', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_10', 'intermediate', 'dense', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_10', 'intermediate', 'dense', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_10', 'output', 'LayerNorm', 'beta']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_10', 'output', 'LayerNorm', 'gamma']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_10', 'output', 'dense', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_10', 'output', 'dense', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_11', 'attention', 'output', 'LayerNorm', 'beta']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_11', 'attention', 'output', 'LayerNorm', 'gamma']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_11', 'attention', 'output', 'dense', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_11', 'attention', 'output', 'dense', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_11', 'attention', 'self', 'key', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_11', 'attention', 'self', 'key', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_11', 'attention', 'self', 'query', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_11', 'attention', 'self', 'query', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_11', 'attention', 'self', 'value', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_11', 'attention', 'self', 'value', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_11', 'intermediate', 'dense', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_11', 'intermediate', 'dense', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_11', 'output', 'LayerNorm', 'beta']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_11', 'output', 'LayerNorm', 'gamma']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_11', 'output', 'dense', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_11', 'output', 'dense', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'attention', 'output', 'LayerNorm', 'beta']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'attention', 'output', 'LayerNorm', 'gamma']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'attention', 'output', 'dense', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'attention', 'output', 'dense', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'attention', 'self', 'key', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'attention', 'self', 'key', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'attention', 'self', 'query', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'attention', 'self', 'query', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'attention', 'self', 'value', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'attention', 'self', 'value', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'intermediate', 'dense', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'intermediate', 'dense', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'output', 'LayerNorm', 'beta']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'output', 'LayerNorm', 'gamma']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'output', 'dense', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'output', 'dense', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'attention', 'output', 'LayerNorm', 'beta']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'attention', 'output', 'LayerNorm', 'gamma']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'attention', 'output', 'dense', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'attention', 'output', 'dense', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'attention', 'self', 'key', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'attention', 'self', 'key', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'attention', 'self', 'query', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'attention', 'self', 'query', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'attention', 'self', 'value', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'attention', 'self', 'value', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'intermediate', 'dense', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'intermediate', 'dense', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'output', 'LayerNorm', 'beta']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'output', 'LayerNorm', 'gamma']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'output', 'dense', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'output', 'dense', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_4', 'attention', 'output', 'LayerNorm', 'beta']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_4', 'attention', 'output', 'LayerNorm', 'gamma']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_4', 'attention', 'output', 'dense', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_4', 'attention', 'output', 'dense', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_4', 'attention', 'self', 'key', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_4', 'attention', 'self', 'key', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_4', 'attention', 'self', 'query', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_4', 'attention', 'self', 'query', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_4', 'attention', 'self', 'value', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_4', 'attention', 'self', 'value', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_4', 'intermediate', 'dense', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_4', 'intermediate', 'dense', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_4', 'output', 'LayerNorm', 'beta']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_4', 'output', 'LayerNorm', 'gamma']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_4', 'output', 'dense', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_4', 'output', 'dense', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_5', 'attention', 'output', 'LayerNorm', 'beta']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_5', 'attention', 'output', 'LayerNorm', 'gamma']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_5', 'attention', 'output', 'dense', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_5', 'attention', 'output', 'dense', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_5', 'attention', 'self', 'key', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_5', 'attention', 'self', 'key', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_5', 'attention', 'self', 'query', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_5', 'attention', 'self', 'query', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_5', 'attention', 'self', 'value', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_5', 'attention', 'self', 'value', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_5', 'intermediate', 'dense', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_5', 'intermediate', 'dense', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_5', 'output', 'LayerNorm', 'beta']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_5', 'output', 'LayerNorm', 'gamma']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_5', 'output', 'dense', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_5', 'output', 'dense', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_6', 'attention', 'output', 'LayerNorm', 'beta']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_6', 'attention', 'output', 'LayerNorm', 'gamma']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_6', 'attention', 'output', 'dense', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_6', 'attention', 'output', 'dense', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_6', 'attention', 'self', 'key', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_6', 'attention', 'self', 'key', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_6', 'attention', 'self', 'query', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_6', 'attention', 'self', 'query', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_6', 'attention', 'self', 'value', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_6', 'attention', 'self', 'value', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_6', 'intermediate', 'dense', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_6', 'intermediate', 'dense', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_6', 'output', 'LayerNorm', 'beta']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_6', 'output', 'LayerNorm', 'gamma']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_6', 'output', 'dense', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_6', 'output', 'dense', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_7', 'attention', 'output', 'LayerNorm', 'beta']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_7', 'attention', 'output', 'LayerNorm', 'gamma']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_7', 'attention', 'output', 'dense', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_7', 'attention', 'output', 'dense', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_7', 'attention', 'self', 'key', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_7', 'attention', 'self', 'key', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_7', 'attention', 'self', 'query', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_7', 'attention', 'self', 'query', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_7', 'attention', 'self', 'value', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_7', 'attention', 'self', 'value', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_7', 'intermediate', 'dense', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_7', 'intermediate', 'dense', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_7', 'output', 'LayerNorm', 'beta']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_7', 'output', 'LayerNorm', 'gamma']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_7', 'output', 'dense', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_7', 'output', 'dense', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_8', 'attention', 'output', 'LayerNorm', 'beta']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_8', 'attention', 'output', 'LayerNorm', 'gamma']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_8', 'attention', 'output', 'dense', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_8', 'attention', 'output', 'dense', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_8', 'attention', 'self', 'key', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_8', 'attention', 'self', 'key', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_8', 'attention', 'self', 'query', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_8', 'attention', 'self', 'query', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_8', 'attention', 'self', 'value', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_8', 'attention', 'self', 'value', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_8', 'intermediate', 'dense', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_8', 'intermediate', 'dense', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_8', 'output', 'LayerNorm', 'beta']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_8', 'output', 'LayerNorm', 'gamma']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_8', 'output', 'dense', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_8', 'output', 'dense', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_9', 'attention', 'output', 'LayerNorm', 'beta']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_9', 'attention', 'output', 'LayerNorm', 'gamma']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_9', 'attention', 'output', 'dense', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_9', 'attention', 'output', 'dense', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_9', 'attention', 'self', 'key', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_9', 'attention', 'self', 'key', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_9', 'attention', 'self', 'query', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_9', 'attention', 'self', 'query', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_9', 'attention', 'self', 'value', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_9', 'attention', 'self', 'value', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_9', 'intermediate', 'dense', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_9', 'intermediate', 'dense', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_9', 'output', 'LayerNorm', 'beta']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_9', 'output', 'LayerNorm', 'gamma']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_9', 'output', 'dense', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_9', 'output', 'dense', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'pooler', 'dense', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'pooler', 'dense', 'kernel']\n",
            "Save PyTorch model to biobert_v1.1_pubmed/pytorch_model.bin\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls biobert_v1.1_pubmed/\n",
        "!mv biobert_v1.1_pubmed/bert_config.json biobert_v1.1_pubmed/config.json\n",
        "!ls biobert_v1.1_pubmed/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JA_8biVMC_1I",
        "outputId": "d7f5e034-021e-4b5b-f678-9df24d67f819"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "bert_config.json\t\t\tmodel.ckpt-1000000.meta\n",
            "model.ckpt-1000000.data-00000-of-00001\tpytorch_model.bin\n",
            "model.ckpt-1000000.index\t\tvocab.txt\n",
            "config.json\t\t\t\tmodel.ckpt-1000000.meta\n",
            "model.ckpt-1000000.data-00000-of-00001\tpytorch_model.bin\n",
            "model.ckpt-1000000.index\t\tvocab.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "model = BertModel.from_pretrained('biobert_v1.1_pubmed')"
      ],
      "metadata": {
        "id": "QzxZ4HE-Cy6u"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### LOAD DATA"
      ],
      "metadata": {
        "id": "H9aEWomvEkpx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_path = \"/content/PredictedLabels_WithAL.csv\"\n",
        "test_path = \"/content/ManualAnnotatedData.csv\"\n",
        "dfHandler = dfutilities.dfUtil()\n",
        "df_train,df_test,log = dfHandler.handle_data(train_path,test_path)\n",
        "log"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KXp4fbaaEKLF",
        "outputId": "9c7e8ae0-c519-493f-cfc9-d164fd431595"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Length of Train Dataset : 60123',\n",
              " 'Length of Test Dataset : 3200',\n",
              " 'Number of Null Values in Train Dataset : 0',\n",
              " 'Number of Null Values in Test Dataset : 0']"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Tokenize the data"
      ],
      "metadata": {
        "id": "1b63vYoNGfkW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "description = list(df_train['CleanText'])\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained('biobert_v1.1_pubmed', do_lower_case=True)\n",
        "\n",
        "tokenized_texts = list(map(lambda t: ['[CLS]']+tokenizer.tokenize(t)+['[SEP]'] , description))\n",
        "print (\"Tokenize the first sentence:\")\n",
        "print (tokenized_texts[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U_oINeaCFaKZ",
        "outputId": "4b8836a0-ea96-4f6e-e6a8-3be25a456b8c"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokenize the first sentence:\n",
            "['[CLS]', '.', '.', '.', 'and', 'there', 'is', 'potential', 'risk', 'of', 'f', '##etal', 'harm', 'from', 'te', '##ri', '##f', '##lu', '##no', '##mi', '##de', 'per', 'pre', '##s', '##cribing', 'information', 'so', 'its', 'important', 'that', 'ch', '##els', '##ea', 'continuous', 'her', 'highly', 'effective', 'and', 'reliable', 'con', '##tra', '##ception', '.', 'te', '##ri', '##f', '##lu', '##no', '##mi', '##de', 'use', 'requires', 'liver', 'enzyme', 'monitoring', 'for', 'he', '##pa', '##to', '##to', '##xi', '##city', '.', '[SEP]']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "classes = list(df_train['Label'].astype(int))"
      ],
      "metadata": {
        "id": "mInHyGL6Gml0"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MAX_LEN = 100\n",
        "\n",
        "input_ids = pad_sequences(list(map(tokenizer.convert_tokens_to_ids, tokenized_texts)),\n",
        "                          maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")"
      ],
      "metadata": {
        "id": "nWe_sDVKGpWs"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_ids = [tokenizer.convert_tokens_to_ids(x) for x in tokenized_texts]\n",
        "input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")"
      ],
      "metadata": {
        "id": "m3tcuJLTGsTP"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "attention_masks = []\n",
        "\n",
        "for seq in input_ids:\n",
        "  seq_mask = [float(i>0) for i in seq]\n",
        "  attention_masks.append(seq_mask)"
      ],
      "metadata": {
        "id": "Ai4XD7l8GuKz"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 16\n",
        "\n",
        "# Use train_test_split to split our data into train and validation sets for training\n",
        "train_inputs, validation_inputs, train_labels, validation_labels = train_test_split(input_ids, classes, \n",
        "                                                            random_state=2020, test_size=0.25)"
      ],
      "metadata": {
        "id": "qNDYK54TGvAl"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(validation_inputs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gqEXUNsiGx4n",
        "outputId": "5d382e6e-2d17-4c0c-8ef4-0fdc67b485cc"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "15031"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_masks, validation_masks, _, _ = train_test_split(attention_masks, input_ids,\n",
        "                                             random_state=2020, test_size=0.25)\n",
        "                                             \n",
        "# Convert all of our data into torch tensors, the required datatype for our model\n",
        "train_inputs = torch.tensor(train_inputs)\n",
        "validation_inputs = torch.tensor(validation_inputs)\n",
        "train_labels = torch.tensor(train_labels)\n",
        "validation_labels = torch.tensor(validation_labels)\n",
        "train_masks = torch.tensor(train_masks)\n",
        "validation_masks = torch.tensor(validation_masks)\n",
        "\n",
        "# Create an iterator of our data with torch DataLoader \n",
        "train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
        "train_sampler = RandomSampler(train_data)\n",
        "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
        "validation_data = TensorDataset(validation_inputs, validation_masks, validation_labels)\n",
        "validation_sampler = SequentialSampler(validation_data)\n",
        "validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)"
      ],
      "metadata": {
        "id": "Ke-6jMCaG4rn"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = BertForSequenceClassification.from_pretrained(\"biobert_v1.1_pubmed\", num_labels=2)#binary classification\n",
        "model.cuda()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gyv7eYg5G8rQ",
        "outputId": "71c9e3e2-0df6-43f2-be92-a814eb63fe21"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BertForSequenceClassification(\n",
              "  (bert): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(28996, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): BertLayerNorm()\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): BertLayerNorm()\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (1): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): BertLayerNorm()\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (2): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): BertLayerNorm()\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (3): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): BertLayerNorm()\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (4): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): BertLayerNorm()\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (5): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): BertLayerNorm()\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (6): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): BertLayerNorm()\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (7): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): BertLayerNorm()\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (8): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): BertLayerNorm()\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (9): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): BertLayerNorm()\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (10): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): BertLayerNorm()\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (11): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): BertLayerNorm()\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): BertPooler(\n",
              "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (dropout): Dropout(p=0.1, inplace=False)\n",
              "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#BERT fine-tuning parameters\n",
        "param_optimizer = list(model.named_parameters())\n",
        "no_decay = ['bias', 'gamma', 'beta']\n",
        "optimizer_grouped_parameters = [\n",
        "    {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n",
        "     'weight_decay_rate': 0.01},\n",
        "    {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n",
        "     'weight_decay_rate': 0.0}\n",
        "]\n",
        "\n",
        "optimizer = BertAdam(optimizer_grouped_parameters,\n",
        "                     lr=2e-5,\n",
        "                     warmup=.1)\n",
        "\n",
        "# Function to calculate the accuracy of our predictions vs labels\n",
        "def flat_accuracy(preds, labels):\n",
        "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
        "    labels_flat = labels.flatten()\n",
        "    return np.sum(pred_flat == labels_flat) / len(labels_flat)\n",
        "\n",
        "torch.cuda.empty_cache() \n",
        "# Store our loss and accuracy for plotting\n",
        "train_loss_set = []\n",
        "# Number of training epochs \n",
        "epochs = 4\n",
        "\n",
        "# BERT training loop\n",
        "for _ in trange(epochs, desc=\"Epoch\"):  \n",
        "  \n",
        "  ## TRAINING\n",
        "  model.train()  \n",
        "  # Tracking variables\n",
        "  tr_loss = 0\n",
        "  nb_tr_examples, nb_tr_steps = 0, 0\n",
        "  # Train the data for one epoch\n",
        "  for step, batch in enumerate(train_dataloader):\n",
        "    # Add batch to GPU\n",
        "    batch = tuple(t.to(device) for t in batch)\n",
        "    # Unpack the inputs from our dataloader\n",
        "    b_input_ids, b_input_mask, b_labels = batch\n",
        "    # Clear out the gradients (by default they accumulate)\n",
        "    optimizer.zero_grad()\n",
        "    # Forward pass\n",
        "    loss = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask, labels=b_labels)\n",
        "    train_loss_set.append(loss.item())    \n",
        "    # Backward pass\n",
        "    loss.backward()\n",
        "    # Update parameters and take a step using the computed gradient\n",
        "    optimizer.step()\n",
        "    # Update tracking variables\n",
        "    tr_loss += loss.item()\n",
        "    nb_tr_examples += b_input_ids.size(0)\n",
        "    nb_tr_steps += 1\n",
        "  print(\"Train loss: {}\".format(tr_loss/nb_tr_steps))\n",
        "  ## VALIDATION\n",
        "\n",
        "  # Put model in evaluation mode\n",
        "  model.eval()\n",
        "  # Tracking variables \n",
        "  eval_loss, eval_accuracy = ples = 0, 0\n",
        "  nb_eval_steps, nb_eval_examples = 0, 0\n",
        "  # Evaluate data for one epoch\n",
        "  for batch in validation_dataloader:\n",
        "    # Add batch to GPU\n",
        "    batch = tuple(t.to(device) for t in batch)\n",
        "    # Unpack the inputs from our dataloader\n",
        "    b_input_ids, b_input_mask, b_labels = batch\n",
        "    # Telling the model not to compute or store gradients, saving memory and speeding up validation\n",
        "    with torch.no_grad():\n",
        "      # Forward pass, calculate logit predictions\n",
        "      logits = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)    \n",
        "    # Move logits and labels to CPU\n",
        "    logits = logits.detach().cpu().numpy()\n",
        "    label_ids = b_labels.to('cpu').numpy()\n",
        "    tmp_eval_accuracy = flat_accuracy(logits, label_ids)    \n",
        "    eval_accuracy += tmp_eval_accuracy\n",
        "    nb_eval_steps += 1\n",
        "  print(\"Validation Accuracy: {}\".format(eval_accuracy/nb_eval_steps))\n",
        "\n",
        "# plot training performance\n",
        "plt.figure(figsize=(15,8))\n",
        "plt.title(\"Training loss\")\n",
        "plt.xlabel(\"Batch\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.plot(train_loss_set)\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 793
        },
        "id": "2zPE31ZbHCjR",
        "outputId": "220e8c60-8d56-480b-d051-cf64e680c11d"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "t_total value of -1 results in schedule not being applied\n",
            "Epoch:   0%|          | 0/4 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/pytorch_pretrained_bert/optimization.py:275: UserWarning: This overload of add_ is deprecated:\n",
            "\tadd_(Number alpha, Tensor other)\n",
            "Consider using one of the following signatures instead:\n",
            "\tadd_(Tensor other, *, Number alpha) (Triggered internally at  ../torch/csrc/utils/python_arg_parser.cpp:1055.)\n",
            "  next_m.mul_(beta1).add_(1 - beta1, grad)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss: 0.09522832695320585\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEpoch:  25%|██▌       | 1/4 [11:09<33:27, 669.05s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 0.9705262158054712\n",
            "Train loss: 0.045959399152165493\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEpoch:  50%|█████     | 2/4 [22:17<22:17, 668.69s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 0.96893047112462\n",
            "Train loss: 0.020181096831929247\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEpoch:  75%|███████▌  | 3/4 [33:33<11:11, 671.85s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 0.9679996200607903\n",
            "Train loss: 0.012937179480238505\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch: 100%|██████████| 4/4 [44:41<00:00, 670.41s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 0.966270896656535\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1080x576 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluate the model on test data"
      ],
      "metadata": {
        "id": "DK78IihAPAHB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "descr = df_test['CleanText'].to_list()\n",
        "tokenized_texts = list(map(lambda t: ['[CLS]']+tokenizer.tokenize(t)+['[SEP]'] , descr))\n",
        "MAX_LEN = 100\n",
        "\n",
        "input_ids = pad_sequences(list(map(tokenizer.convert_tokens_to_ids, tokenized_texts)),\n",
        "                          maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
        "input_ids = [tokenizer.convert_tokens_to_ids(x) for x in tokenized_texts]\n",
        "input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")"
      ],
      "metadata": {
        "id": "ZyysRLgzPEM7"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "attention_masks = []\n",
        "\n",
        "for seq in input_ids:\n",
        "  seq_mask = [float(i>0) for i in seq]\n",
        "  attention_masks.append(seq_mask)"
      ],
      "metadata": {
        "id": "FBD6LxUqPLuv"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "classes = df_test['Label'].to_list()"
      ],
      "metadata": {
        "id": "EoQ_Ns-vPOwL"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert all of our data into torch tensors, the required datatype for our model\n",
        "\n",
        "test_inputs = torch.tensor(input_ids)\n",
        "test_labels = torch.tensor(classes)\n",
        "test_masks = torch.tensor(attention_masks)\n",
        "\n",
        "# Create an iterator of our data with torch DataLoader \n",
        "test_data = TensorDataset(test_inputs, test_masks, test_labels)\n",
        "test_sampler = RandomSampler(test_data)\n",
        "test_dataloader = DataLoader(test_data, sampler=test_sampler, batch_size=batch_size)"
      ],
      "metadata": {
        "id": "yws9eMrzPTRI"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3_XFDHVrPZvl",
        "outputId": "f9574d4e-72d7-410e-9f92-ac6b71ff9e33"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BertForSequenceClassification(\n",
              "  (bert): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(28996, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): BertLayerNorm()\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): BertLayerNorm()\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (1): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): BertLayerNorm()\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (2): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): BertLayerNorm()\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (3): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): BertLayerNorm()\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (4): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): BertLayerNorm()\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (5): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): BertLayerNorm()\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (6): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): BertLayerNorm()\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (7): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): BertLayerNorm()\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (8): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): BertLayerNorm()\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (9): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): BertLayerNorm()\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (10): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): BertLayerNorm()\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (11): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): BertLayerNorm()\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): BertPooler(\n",
              "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (dropout): Dropout(p=0.1, inplace=False)\n",
              "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Tracking variables \n",
        "predictions , true_labels = [], []\n",
        "\n",
        "# Predict \n",
        "for batch in test_dataloader:\n",
        "  # Add batch to GPU\n",
        "  batch = tuple(t.to(device) for t in batch)\n",
        "  \n",
        "  # Unpack the inputs from our dataloader\n",
        "  b_input_ids, b_input_mask, b_labels = batch\n",
        "  \n",
        "  # Telling the model not to compute or store gradients, saving memory and \n",
        "  # speeding up prediction\n",
        "  with torch.no_grad():\n",
        "      # Forward pass, calculate logit predictions\n",
        "      logits = model(b_input_ids, token_type_ids=None, \n",
        "                      attention_mask=b_input_mask)\n",
        "\n",
        "\n",
        "  #logits = outputs[0]\n",
        "  # Move logits and labels to CPU\n",
        "  logits = logits.detach().cpu().numpy()\n",
        "  #logits = outputs.cpu().numpy()\n",
        "  label_ids = b_labels.to('cpu').numpy()\n",
        "  \n",
        "  # Store predictions and true labels\n",
        "  predictions.append(logits)\n",
        "  true_labels.append(label_ids)"
      ],
      "metadata": {
        "id": "jyZbayCfPeVL"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Combine the results across all batches. \n",
        "flat_predictions = np.concatenate(predictions, axis=0)\n",
        "\n",
        "# For each sample, pick the label (0 or 1) with the higher score.\n",
        "flat_predictions = np.argmax(flat_predictions, axis=1).flatten()\n",
        "\n",
        "# Combine the correct labels for each batch into a single list.\n",
        "flat_true_labels = np.concatenate(true_labels, axis=0)"
      ],
      "metadata": {
        "id": "abiLKJg1PhIS"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ypred = flat_predictions \n",
        "ytrue = flat_true_labels"
      ],
      "metadata": {
        "id": "SlSCu3u8PiH6"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "categories = ['Others', 'Patient']\n",
        "cm = confusion_matrix(ypred,ytrue)\n",
        "make_confusion_matrix(cm, cbar=False,\n",
        "                      #group_names=labels,\n",
        "                      categories=categories, \n",
        "                      cmap='Blues')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 337
        },
        "id": "eZIre5FXSlsl",
        "outputId": "29950108-48fe-403e-e375-baf514bceb98"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}